{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现互信息和左右熵的新词发现（Python3）\n",
    "> 参考：\n",
    "> * [python3实现互信息和左右熵的新词发现](https://www.jianshu.com/p/e9313fd692ef)\n",
    "> * [Github Chinese_segment_augment](https://github.com/zhanzecheng/Chinese_segment_augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词核心实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    建立字典树的节点\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, char):\n",
    "        self.char = char\n",
    "        # 记录是否完成\n",
    "        self.word_finish = False\n",
    "        # 用来计数\n",
    "        self.count = 0\n",
    "        # 用来存放节点\n",
    "        self.child = []\n",
    "        # 方便计算 左右熵\n",
    "        # 判断是否是后缀（标识后缀用的，也就是记录 b->c->a 变换后的标记）\n",
    "        self.isback = False\n",
    "\n",
    "\n",
    "class TrieNode(object):\n",
    "    \"\"\"\n",
    "    建立前缀树，并且包含统计词频，计算左右熵，计算互信息的方法\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, node, data=None, PMI_limit=20):\n",
    "        \"\"\"\n",
    "        初始函数，data为外部词频数据集\n",
    "        :param node:\n",
    "        :param data:\n",
    "        \"\"\"\n",
    "        self.root = Node(node)\n",
    "        self.PMI_limit = PMI_limit\n",
    "        if not data:\n",
    "            return\n",
    "        node = self.root\n",
    "        for key, values in data.items():\n",
    "            new_node = Node(key)\n",
    "            new_node.count = int(values)\n",
    "            new_node.word_finish = True\n",
    "            node.child.append(new_node)\n",
    "\n",
    "    def add(self, word):\n",
    "        \"\"\"\n",
    "        添加节点，对于左熵计算时，这里采用了一个trick，用a->b<-c 来表示 cba\n",
    "        具体实现是利用 self.isback 来进行判断\n",
    "        :param word:\n",
    "        :return:  相当于对 [a, b, c] a->b->c, [b, c, a] b->c->a\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        # 正常加载\n",
    "        for count, char in enumerate(word):\n",
    "            found_in_child = False\n",
    "            # 在节点中找字符\n",
    "            for child in node.child:\n",
    "                if char == child.char:\n",
    "                    node = child\n",
    "                    found_in_child = True\n",
    "                    break\n",
    "\n",
    "            # 顺序在节点后面添加节点。 a->b->c\n",
    "            if not found_in_child:\n",
    "                new_node = Node(char)\n",
    "                node.child.append(new_node)\n",
    "                node = new_node\n",
    "\n",
    "            # 判断是否是最后一个节点，这个词每出现一次就+1\n",
    "            if count == len(word) - 1:\n",
    "                node.count += 1\n",
    "                node.word_finish = True\n",
    "\n",
    "        # 建立后缀表示\n",
    "        length = len(word)\n",
    "        node = self.root\n",
    "        if length == 3:\n",
    "            word = list(word)\n",
    "            word[0], word[1], word[2] = word[1], word[2], word[0]\n",
    "\n",
    "            for count, char in enumerate(word):\n",
    "                found_in_child = False\n",
    "                # 在节点中找字符（不是最后的后缀词）\n",
    "                if count != length - 1:\n",
    "                    for child in node.child:\n",
    "                        if char == child.char:\n",
    "                            node = child\n",
    "                            found_in_child = True\n",
    "                            break\n",
    "                else:\n",
    "                    # 由于初始化的 isback 都是 False， 所以在追加 word[2] 后缀肯定找不到\n",
    "                    for child in node.child:\n",
    "                        if char == child.char and child.isback:\n",
    "                            node = child\n",
    "                            found_in_child = True\n",
    "                            break\n",
    "\n",
    "                # 顺序在节点后面添加节点。 b->c->a\n",
    "                if not found_in_child:\n",
    "                    new_node = Node(char)\n",
    "                    node.child.append(new_node)\n",
    "                    node = new_node\n",
    "\n",
    "                # 判断是否是最后一个节点，这个词每出现一次就+1\n",
    "                if count == len(word) - 1:\n",
    "                    node.count += 1\n",
    "                    node.isback = True\n",
    "                    node.word_finish = True\n",
    "\n",
    "    def search_one(self):\n",
    "        \"\"\"\n",
    "        计算互信息: 寻找一阶共现，并返回词概率\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        node = self.root\n",
    "        if not node.child:\n",
    "            return False, 0\n",
    "\n",
    "        # 计算 1 gram 总的出现次数\n",
    "        total = 0\n",
    "        for child in node.child:\n",
    "            if child.word_finish is True:\n",
    "                total += child.count\n",
    "\n",
    "        # 计算 当前词 占整体的比例\n",
    "        for child in node.child:\n",
    "            if child.word_finish is True:\n",
    "                result[child.char] = child.count / total\n",
    "        return result, total\n",
    "\n",
    "    def search_bi(self):\n",
    "        \"\"\"\n",
    "        计算互信息: 寻找二阶共现，并返回 log2( P(X,Y) / (P(X) * P(Y)) 和词概率\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        node = self.root\n",
    "        if not node.child:\n",
    "            return False, 0\n",
    "\n",
    "        total = 0\n",
    "        # 1 grem 各词的占比，和 1 grem 的总次数\n",
    "        one_dict, total_one = self.search_one()\n",
    "        for child in node.child:\n",
    "            for ch in child.child:\n",
    "                if ch.word_finish is True:\n",
    "                    total += ch.count\n",
    "\n",
    "        for child in node.child:\n",
    "            for ch in child.child:\n",
    "                if ch.word_finish is True:\n",
    "                    # 互信息值越大，说明 a,b 两个词相关性越大\n",
    "                    PMI = math.log(max(ch.count, 1), 2) - math.log(total, 2) - math.log(one_dict[child.char],\n",
    "                                                                                        2) - math.log(one_dict[ch.char],\n",
    "                                                                                                      2)\n",
    "                    # 这里做了PMI阈值约束\n",
    "                    if PMI > self.PMI_limit:\n",
    "                        # 例如: dict{ \"a_b\": (PMI, 出现概率), .. }\n",
    "                        result[child.char + '_' + ch.char] = (PMI, ch.count / total)\n",
    "        return result\n",
    "\n",
    "    def search_left(self):\n",
    "        \"\"\"\n",
    "        寻找左频次\n",
    "        统计左熵， 并返回左熵 (bc - a 这个算的是 abc|bc 所以是左熵)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        node = self.root\n",
    "        if not node.child:\n",
    "            return False, 0\n",
    "\n",
    "        for child in node.child:\n",
    "            for cha in child.child:\n",
    "                total = 0\n",
    "                p = 0.0\n",
    "                for ch in cha.child:\n",
    "                    if ch.word_finish is True and ch.isback:\n",
    "                        total += ch.count\n",
    "                for ch in cha.child:\n",
    "                    if ch.word_finish is True and ch.isback:\n",
    "                        p += (ch.count / total) * math.log(ch.count / total, 2)\n",
    "                # 计算的是信息熵\n",
    "                result[child.char + cha.char] = -p\n",
    "        return result\n",
    "\n",
    "    def search_right(self):\n",
    "        \"\"\"\n",
    "        寻找右频次\n",
    "        统计右熵，并返回右熵 (ab - c 这个算的是 abc|ab 所以是右熵)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        node = self.root\n",
    "        if not node.child:\n",
    "            return False, 0\n",
    "\n",
    "        for child in node.child:\n",
    "            for cha in child.child:\n",
    "                total = 0\n",
    "                p = 0.0\n",
    "                for ch in cha.child:\n",
    "                    if ch.word_finish is True and not ch.isback:\n",
    "                        total += ch.count\n",
    "                for ch in cha.child:\n",
    "                    if ch.word_finish is True and not ch.isback:\n",
    "                        p += (ch.count / total) * math.log(ch.count / total, 2)\n",
    "                # 计算的是信息熵\n",
    "                result[child.char + cha.char] = -p\n",
    "        return result\n",
    "\n",
    "    def find_word(self, N):\n",
    "        # 通过搜索得到互信息\n",
    "        # 例如: dict{ \"a_b\": (PMI, 出现概率), .. }\n",
    "        bi = self.search_bi()\n",
    "        # 通过搜索得到左右熵\n",
    "        left = self.search_left()\n",
    "        right = self.search_right()\n",
    "        result = {}\n",
    "        for key, values in bi.items():\n",
    "            d = \"\".join(key.split('_'))\n",
    "            # 计算公式 score = PMI + min(左熵， 右熵) => 熵越小，说明越有序，这词再一次可能性更大！\n",
    "            result[key] = (values[0] + min(left[d], right[d])) * values[1]\n",
    "\n",
    "        # 按照 大到小倒序排列，value 值越大，说明是组合词的概率越大\n",
    "        # result变成 => [('世界卫生_大会', 0.4380419441616299), ('蔡_英文', 0.28882968751888893) ..]\n",
    "        result = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"result: \", result)\n",
    "        dict_list = [result[0][0]]\n",
    "        # print(\"dict_list: \", dict_list)\n",
    "        add_word = {}\n",
    "        new_word = \"\".join(dict_list[0].split('_'))\n",
    "        # 获得概率\n",
    "        add_word[new_word] = result[0][1]\n",
    "\n",
    "        # 取前5个\n",
    "        # [('蔡_英文', 0.28882968751888893), ('民进党_当局', 0.2247420989996931), ('陈时_中', 0.15996145099751344), ('九二_共识', 0.14723726297223602)]\n",
    "        for d in result[1: N]:\n",
    "            flag = True\n",
    "            for tmp in dict_list:\n",
    "                pre = tmp.split('_')[0]\n",
    "                # 新出现单词后缀，再老词的前缀中 or 如果发现新词，出现在列表中; 则跳出循环 \n",
    "                # 前面的逻辑是： 如果A和B组合，那么B和C就不能组合(这个逻辑有点问题)，例如：`蔡_英文` 出现，那么 `英文_也` 这个不是新词\n",
    "                # 疑惑: **后面的逻辑，这个是完全可能出现，毕竟没有重复**\n",
    "                if d[0].split('_')[-1] == pre or \"\".join(tmp.split('_')) in \"\".join(d[0].split('_')):\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                new_word = \"\".join(d[0].split('_'))\n",
    "                add_word[new_word] = d[1]\n",
    "                dict_list.append(d[0])\n",
    "\n",
    "        return result, add_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    with open('/home/centos/leon/machine_learning_jupyter/nlp/新词发现/data/stopword.txt', 'r') as f:\n",
    "        stopword = [line.strip() for line in f]\n",
    "    return set(stopword)\n",
    "\n",
    "\n",
    "def generate_ngram(input_list, n):\n",
    "    result = []\n",
    "    for i in range(1, n+1):\n",
    "        result.extend(zip(*[input_list[j:] for j in range(i)]))\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_dictionary(filename):\n",
    "    \"\"\"\n",
    "    加载外部词频记录\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    word_freq = {}\n",
    "    print('------> 加载外部词集')\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                line_list = line.strip().split(' ')\n",
    "                # 规定最少词频\n",
    "                if int(line_list[1]) > 2:\n",
    "                    word_freq[line_list[0]] = line_list[1]\n",
    "            except IndexError as e:\n",
    "                print(line)\n",
    "                continue\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "def save_model(model, filename):\n",
    "    with open(filename, 'wb') as fw:\n",
    "        pickle.dump(model, fw)\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as fr:\n",
    "        model = pickle.load(fr)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "\n",
    "\n",
    "basedir = '/home/centos/leon/machine_learning_jupyter/nlp/新词发现'\n",
    "\n",
    "\n",
    "def load_data(filename, stopwords):\n",
    "    \"\"\"\n",
    "\n",
    "    :param filename:\n",
    "    :param stopwords:\n",
    "    :return: 二维数组,[[句子1分词list], [句子2分词list],...,[句子n分词list]]\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            word_list = [x for x in jieba.cut(line.strip(), cut_all=False) if x not in stopwords]\n",
    "            data.append(word_list)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data_2_root(data):\n",
    "    print('------> 插入节点')\n",
    "    for word_list in data:\n",
    "        # tmp 表示每一行自由组合后的结果（n gram）\n",
    "        # tmp: [['它'], ['是'], ['小'], ['狗'], ['它', '是'], ['是', '小'], ['小', '狗'], ['它', '是', '小'], ['是', '小', '狗']]\n",
    "        ngrams = generate_ngram(word_list, 3)\n",
    "        for d in ngrams:\n",
    "            root.add(d)\n",
    "    print('------> 插入成功')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_name = basedir + \"/data/root.pkl\"\n",
    "    stopwords = get_stopwords()\n",
    "    if os.path.exists(root_name):\n",
    "        root = load_model(root_name)\n",
    "    else:\n",
    "        dict_name = basedir + '/data/dict.txt'\n",
    "        word_freq = load_dictionary(dict_name)\n",
    "        root = TrieNode('*', word_freq)\n",
    "        save_model(root, root_name)\n",
    "\n",
    "    # 加载新的文章\n",
    "    filename = basedir + '/data/demo.txt'\n",
    "    data = load_data(filename, stopwords)\n",
    "    # 将新的文章插入到Root中\n",
    "    load_data_2_root(data)\n",
    "\n",
    "    # 定义取TOP5个\n",
    "    topN = 5\n",
    "    result, add_word = root.find_word(topN)\n",
    "    # 如果想要调试和选择其他的阈值，可以print result来调整\n",
    "    # print(\"\\n----\\n\", result)\n",
    "    print(\"\\n----\\n\", '增加了 %d 个新词, 词语和得分分别为: \\n' % len(add_word))\n",
    "    print('#############################')\n",
    "    for word, score in add_word.items():\n",
    "        print(word + ' ---->  ', score)\n",
    "    print('#############################')\n",
    "\n",
    "    # 前后效果对比\n",
    "    test_sentence = '蔡英文在昨天应民进党当局的邀请，准备和陈时中一道前往世界卫生大会，和谈有关九二共识问题'\n",
    "    print('添加前：')\n",
    "    print(\"\".join([(x + '/ ') for x in jieba.cut(test_sentence, cut_all=False) if x not in stopwords]))\n",
    "\n",
    "    for word in add_word.keys():\n",
    "        jieba.add_word(word)\n",
    "    print(\"添加后：\")\n",
    "    print(\"\".join([(x + '/ ') for x in jieba.cut(test_sentence, cut_all=False) if x not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
